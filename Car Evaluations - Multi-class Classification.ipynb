{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car evaluations classification\n",
    "\n",
    "## Setup\n",
    "\n",
    "Importing necessary libraries for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import necessary libraries and specify that graphs should be plotted inline. \n",
    "from sklearn import tree,linear_model,neighbors, datasets\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading, exploring & Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the data set are 1728 samples by 7 features.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('car.data', header = None)\n",
    "n_samples, n_features = data.shape\n",
    "print ('The dimensions of the data set are', n_samples, 'samples by', n_features,'features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Maintenance Cost</th>\n",
       "      <th>Number of Doors</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>Size of Luggage Boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>low</td>\n",
       "      <td>med</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>5more</td>\n",
       "      <td>4</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>low</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>med</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>med</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "      <td>3</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>3</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price Maintenance Cost Number of Doors Capacity Size of Luggage Boot  \\\n",
       "1515   low              med               2        2                  med   \n",
       "572   high             high               3        2                  med   \n",
       "1500   low             high           5more        4                  big   \n",
       "1332   low            vhigh               3        4                small   \n",
       "868    med            vhigh               2        2                  med   \n",
       "555   high             high               2        4                  big   \n",
       "871    med            vhigh               2        2                  big   \n",
       "803   high              low               3     more                small   \n",
       "1620   low              low               2        2                small   \n",
       "1449   low             high               3     more                small   \n",
       "\n",
       "     safety Decision  \n",
       "1515    low    unacc  \n",
       "572    high    unacc  \n",
       "1500    low    unacc  \n",
       "1332    low    unacc  \n",
       "868     med    unacc  \n",
       "555     low    unacc  \n",
       "871     med    unacc  \n",
       "803    high      acc  \n",
       "1620    low    unacc  \n",
       "1449    low    unacc  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning names to the columns in the dataset\n",
    "data.columns = ['Price', 'Maintenance Cost', 'Number of Doors', 'Capacity', 'Size of Luggage Boot', 'safety', 'Decision']\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding\n",
    "\n",
    "First converting the ordinal variables into numeric for our analysis. We will also be converting them to categorical data using One Hot encoding for the best model and comparing the performance later.\n",
    "\n",
    "For now, first converting to numerical data because logically the classes can be given an order. And based on some business knowledge we know that vhigh is greater than high and med is greater than low, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label Encoding\n",
    "# Replacing with numerical data as they are ordinal variables\n",
    "data['Price'].replace(('low', 'med', 'high', 'vhigh'), (0, 1, 2, 3), inplace = True)\n",
    "data['Maintenance Cost'].replace(('low', 'med', 'high', 'vhigh'), (0, 1, 2, 3), inplace = True)\n",
    "data['Number of Doors'].replace('5more', 5, inplace = True)\n",
    "data['Capacity'].replace('more', 5, inplace = True)\n",
    "data['safety'].replace(('low', 'med', 'high'), (0, 1, 2), inplace = True)\n",
    "data['Size of Luggage Boot'].replace(('small', 'med', 'big'), (0, 1, 2), inplace = True)\n",
    "data['Decision'].replace(('unacc', 'acc', 'good', 'vgood'), (0, 1, 2, 3), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of the data after Numerical Ordinal encoding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Maintenance Cost</th>\n",
       "      <th>Number of Doors</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>Size of Luggage Boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price  Maintenance Cost Number of Doors Capacity  Size of Luggage Boot  \\\n",
       "1690      0                 0               4        4                     2   \n",
       "619       2                 2               4        5                     2   \n",
       "348       3                 0               2        5                     2   \n",
       "1469      0                 2               4        4                     0   \n",
       "1216      1                 0               3        2                     0   \n",
       "15        3                 3               2        4                     2   \n",
       "178       3                 2               4        4                     2   \n",
       "1583      0                 1               4        4                     2   \n",
       "1204      1                 0               2        4                     2   \n",
       "857       2                 0               5        5                     0   \n",
       "\n",
       "      safety  Decision  \n",
       "1690       1         2  \n",
       "619        1         1  \n",
       "348        0         0  \n",
       "1469       2         1  \n",
       "1216       1         0  \n",
       "15         0         0  \n",
       "178        1         0  \n",
       "1583       2         3  \n",
       "1204       1         2  \n",
       "857        2         1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:  (1728, 6)\n",
      "Shape of y:  (1728,)\n"
     ]
    }
   ],
   "source": [
    "# splitting the dataset into predictors(x) and target(y) variables\n",
    "\n",
    "x = data.iloc[:,:6]\n",
    "y = data.iloc[:, 6]\n",
    "\n",
    "print(\"Shape of x: \", x.shape)\n",
    "print(\"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting 20% aside as testing data for final evaluation of our model\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 45, stratify = y)\n",
    "\n",
    "#setting 20% aside as validation data for cross validation\n",
    "x_train_t, x_train_v, y_train_t, y_train_v = train_test_split(x_train, y_train, test_size = 0.20, random_state = 45, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.200000\n",
       "1    0.200521\n",
       "2    0.202899\n",
       "3    0.200000\n",
       "Name: Decision, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if sampling is stratified\n",
    "y_test.value_counts() / y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "### Modeling\n",
    "\n",
    "As a starting point, we run the default Logistic regression classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 84.15%\n",
      "Testing Accuracy: 86.71%\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[218   8  11   5]\n",
      " [ 12  60   4   1]\n",
      " [  0   0  14   0]\n",
      " [  0   2   3   8]]\n"
     ]
    }
   ],
   "source": [
    "# creating a model\n",
    "# since the classes are not balanced, using class_weight = weighted\n",
    "model = linear_model.LogisticRegression(class_weight=\"balanced\")\n",
    "# feeding the training data into the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predicting the values for x-test\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# finding the training and testing accuracy\n",
    "print(\"Training Accuracy: {0:.2%}\".format(model.score(x_train, y_train)))\n",
    "print(\"Testing Accuracy: {0:.2%}\".format(model.score(x_test, y_test)))\n",
    "print()\n",
    "# printing the confusion Matrix\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters - Nested CV & Grid search\n",
    "\n",
    "To search for the best parameters, we run a nested cross validation with grid search with the following hyper parameters. \n",
    "\n",
    "1. solver : Among the various solvers available for multi-class classification, we see what would be the best for our model - 'newton-cg', 'lbfgs', 'sag', 'saga'\n",
    "2. C : We also check what is an optimal value for regularization strength. The hyper-parameter 'C' is 1 / lambda (which is an inverse of the penalty or regularization strength.\n",
    "\n",
    "We also set the hyper parameters multi_class = auto as it is a multiclass classification problem. And we use penalty = l2 for regularization.\n",
    "\n",
    "I have decided to use scoring as **micro-avergaed f1 score** in the grid search cross validation because:-\n",
    "1. f1 score is a better metric to determine a good model as it is the harmonic mean of precision and recall.\n",
    "2. micro averaged f1 score tunes grid search to predict well for all classes as it calculates metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \\\n",
    "              'C':[1e-4,0.001,.009,0.01,.09,1,5,10,25,100,1000,1e4]}\n",
    "\n",
    "clf = linear_model.LogisticRegression(class_weight=\"balanced\", multi_class='auto', penalty = 'l2', random_state=45)\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_logit = GridSearchCV(clf, grid_values, cv = inner_cv, scoring='f1_micro',n_jobs=-1)\n",
    "grid_logit.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_logit, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 83.00% +/- 1.02%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'C': 25, 'solver': 'newton-cg'}\n",
      "\n",
      "The best logistic regression classifier is :-\n",
      " LogisticRegression(C=25, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='auto', n_jobs=None, penalty='l2', random_state=45,\n",
      "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)\n",
      "Confusion Matrix: - \n",
      " [[208  22   9   3]\n",
      " [  7  64   3   3]\n",
      " [  0   2  11   1]\n",
      " [  0   1   1  11]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91       242\n",
      "           1       0.72      0.83      0.77        77\n",
      "           2       0.46      0.79      0.58        14\n",
      "           3       0.61      0.85      0.71        13\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       346\n",
      "   macro avg       0.69      0.83      0.74       346\n",
      "weighted avg       0.88      0.85      0.86       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_logit.best_params_)\n",
    "print()\n",
    "print (\"The best logistic regression classifier is :-\\n\", grid_logit.best_estimator_)\n",
    "y_pred = grid_logit.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "Logistic regression grid search with nested cross validation tells us that the best parameters are **C: 25, penalty: 'l2', solver: 'newton-cg'**\n",
    "\n",
    "To avoid overfitting, we use regularization by a L2 penalty and a C parameter value = 25.\n",
    "\n",
    "\n",
    "With this, we get an accuracy of 83.00% +/- 1.02% which is quite low, and the standard deviation in accuracy is high, which indicates that the model is not generalized well.\n",
    "\n",
    "The prevision and recall vary for various classes and hence it is meaningful to ook at the f1-scores, which is good for classe unacceptable (0), but low for the other classes. The micro avg f1-score is 85% between all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Decision Trees \n",
    "\n",
    "### Modeling\n",
    "\n",
    "Initialized the default Decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Testing Accuracy:  0.9913294797687862\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       242\n",
      "           1       0.97      0.99      0.98        77\n",
      "           2       1.00      1.00      1.00        14\n",
      "           3       0.93      1.00      0.96        13\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       346\n",
      "   macro avg       0.98      0.99      0.98       346\n",
      "weighted avg       0.99      0.99      0.99       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a model\n",
    "model = tree.DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "# feeding the training data into the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predicting the values for x-test\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# finding the training and testing accuracy\n",
    "print(\"Training Accuracy: \",model.score(x_train, y_train))\n",
    "print(\"Testing Accuracy: \", model.score(x_test, y_test))\n",
    "\n",
    "# printing the confusion Matrix\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=['unacc', 'acc', 'good', 'vgood'],\n",
    "#                      title='Confusion matrix - Decision Tree')\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a trainign accuracy of 100% and testing accuracy of 98%. Even though the difference is not that high, a traning accuracy of 100% is an indication of overfitting, so looking at tuning the model so that the trianing accuracy is close to the testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyper-parameters\n",
    "\n",
    "Now we see what is the best max-depth for maximizing accuracy in the test data.\n",
    "\n",
    "The max-depth of a tree defines the number of features used to classify the data. Using very few features would result in underfitting and using a lot of features would lead to over-fitting.\n",
    "\n",
    "We also look at both criterions gini and entropy to see which gives us a better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "complexity_values = range(1,30)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies=[]\n",
    "\n",
    "for complexity_value in complexity_values:\n",
    "    clf = tree.DecisionTreeClassifier(class_weight=\"balanced\",criterion=\"gini\", max_depth=complexity_value)\n",
    "    test_accuracies.append(clf.fit(x_train, y_train).score(x_test, y_test))\n",
    "    train_accuracies.append(clf.fit(x_train, y_train).score(x_train, y_train))\n",
    "    \n",
    "# We want to plot our results\n",
    "line1, =plt.plot(complexity_values, test_accuracies,label='test_accuracies')\n",
    "line2, =plt.plot(complexity_values, train_accuracies,label='train_accuracies')\n",
    "plt.xlabel(\"max_depth (with gini)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(range(2,30,2))\n",
    "plt.legend((line1, line2), ('test', 'train'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that a max depth = 10 and gini scoring method has a good tradeoff between high accuracy and low variation between training and testing data, but this can be further tuned with grid search nested cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning parameters - Nested CV & Grid search\n",
    "\n",
    "To search for the best parameters, we run a nested cross-validation grid search with the following hyper parameters\n",
    "\n",
    "1. criterion : to find an optimal criterion to calculate impurity of a node\n",
    "2. max_depth : to search for the optimal number of features used for classification. Setting the right value for the max_depth (i.e. the number of features used for classification) is crucial to avoid overfitting or underfitting\n",
    "3. min_samples_leaf : to search for the optimal minimum number of samples required to form a leaf\n",
    "4. min_impurity_decrease : to search for the optimal minimum impurity decrease to split a node. If this split induces a decrease of the impurity greater than or equal to min_impurity_decrease only then the split will happen. This prevents splits where information gain is very low.\n",
    "\n",
    "I have decided to use scoring as micro-avergaed f1 score in the grid search cross validation because:-\n",
    "\n",
    "1. f1 score is a better metric to determine a good model as it is the harmonic mean of precision and recall.\n",
    "2. micro averaged f1 score tunes grid search to predict well for all classes as it calculates metrics globally by counting the total true positives, false negatives and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(criterion = [\"gini\", \"entropy\"], \n",
    "                  max_depth = range(2,10),\n",
    "                  min_samples_leaf = range(2,8),\n",
    "                  min_impurity_decrease = [0,1e-8,1e-7,1e-6,1e-5,1e-4]\n",
    "                 )\n",
    "\n",
    "grid_tree_clf = tree.DecisionTreeClassifier(class_weight=\"balanced\",random_state=45)\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_tree = GridSearchCV(grid_tree_clf, param_grid, cv = inner_cv, scoring='f1_micro', n_jobs = -1)\n",
    "grid_tree.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_tree, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 96.89% +/- 0.74%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'criterion': 'gini', 'max_depth': 9, 'min_impurity_decrease': 0, 'min_samples_leaf': 2}\n",
      "\n",
      "The best decision tree classifier is :-\n",
      " DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=9,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=45,\n",
      "            splitter='best')\n",
      "Confusion Matrix: - \n",
      " [[236   6   0   0]\n",
      " [  1  73   2   1]\n",
      " [  0   0  14   0]\n",
      " [  0   0   0  13]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       242\n",
      "           1       0.92      0.95      0.94        77\n",
      "           2       0.88      1.00      0.93        14\n",
      "           3       0.93      1.00      0.96        13\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       346\n",
      "   macro avg       0.93      0.98      0.95       346\n",
      "weighted avg       0.97      0.97      0.97       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_tree.best_params_)\n",
    "print()\n",
    "print (\"The best decision tree classifier is :-\\n\", grid_tree.best_estimator_)\n",
    "y_pred = grid_tree.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "Decision tree grid search with nested cross validation tells us that the best parameters for decision tree classification are  are criterion: 'gini', max_depth: 9, min_impurity_decrease: 0, min_samples_leaf: 2\n",
    "\n",
    "To avoid overfitting, we use max depth as 9, and we saw a similar pattern in the graph above.\n",
    "\n",
    "With this, we get an accuracy of 96.89% +/- 0.74% which is much higher than the accuracy of logistic regression model. The standard deviation in accuracy is low, which indicates that the model is generalized well.\n",
    "\n",
    "The prevision and recall vary for various classes but not as much as they did in logistic regression but it is still meaningful to look at the f1-scores which are good for most of the classes, but relatively low for class 2 at 93%. The micro avg f1-score is 97% which is the best we have got so far.\n",
    "\n",
    "### Prediction between classes\n",
    "\n",
    "1. Class 0 has a high precision and recall of 100% and 98% respectively. There are only 6 unacceptable cars missclassified  which is okay considering 236 are identified correctly.\n",
    "2. CLass 1 has a good precision and recall. There are 4 cars in class 1 which are misclassified and there are 6 cars classified as 1 which are not. This might be reason for problem.\n",
    "3. Class 2 has 2 cars classified as 2 inaccurately.\n",
    "4. This Model has predicted extremely well for class 3, with all 13 identified correctly (recall = 100%).\n",
    "\n",
    "Therefore, the Decision Tree classifier predicts really well for all classes but slightly lower of 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Nearest Neighbours\n",
    "\n",
    "### Modeling\n",
    "\n",
    "We start by runnng the default KNeighborsClassifier classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.983357452966715\n",
      "Testing Accuracy:  0.9566473988439307\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[237   5   0   0]\n",
      " [  4  73   0   0]\n",
      " [  0   4  10   0]\n",
      " [  0   2   0  11]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       242\n",
      "           1       0.87      0.95      0.91        77\n",
      "           2       1.00      0.71      0.83        14\n",
      "           3       1.00      0.85      0.92        13\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       346\n",
      "   macro avg       0.96      0.87      0.91       346\n",
      "weighted avg       0.96      0.96      0.96       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# feeding the training data into the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predicting the values for x-test\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# finding the training and testing accuracy\n",
    "print(\"Training Accuracy: \",model.score(x_train, y_train))\n",
    "print(\"Testing Accuracy: \", model.score(x_test, y_test))\n",
    "print()\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "# printing the confusion Matrix\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=['unacc', 'acc', 'good', 'vgood'],\n",
    "#                      title='Confusion matrix - KNN')\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a training accuracy of 98% and testing accuracy of 95%. This is a significant differnece between trianing and testing accuracies and might be an indication of overfitting, so looking at tuning the model so that the trianing accuracy is close to the testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters - Nested CV & Grid search\n",
    "\n",
    "To search for the best parameters, we run a grid search with the following hyper parameters\n",
    "\n",
    "weights : To check whether assigneing uniform weights or weights based on distance give a us better model.\n",
    "n_neighbors : As defined earlier, it is crucial to select an optimal number of neighbours to run the classification model.\n",
    "\n",
    "I have decided to use scoring as micro-avergaed f1 score in the grid search cross validation as explained in the previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1,30))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "param_grid = dict(n_neighbors = k_range, weights = weight_options)\n",
    "\n",
    "grid_knn_clf = neighbors.KNeighborsClassifier()\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_knn = GridSearchCV(grid_knn_clf, param_grid, cv = inner_cv, scoring='f1_micro', n_jobs = -1)\n",
    "grid_knn.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_knn, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 94.14% +/- 0.83%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'n_neighbors': 6, 'weights': 'distance'}\n",
      "\n",
      "The best KNN classifier is :-\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='distance')\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[237   5   0   0]\n",
      " [  3  73   1   0]\n",
      " [  0   2  12   0]\n",
      " [  0   2   1  10]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       242\n",
      "           1       0.89      0.95      0.92        77\n",
      "           2       0.86      0.86      0.86        14\n",
      "           3       1.00      0.77      0.87        13\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       346\n",
      "   macro avg       0.93      0.89      0.91       346\n",
      "weighted avg       0.96      0.96      0.96       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_knn.best_params_)\n",
    "print()\n",
    "print (\"The best KNN classifier is :-\\n\", grid_knn.best_estimator_)\n",
    "y_pred = grid_knn.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print()\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "K Nearest Neighbours with nested cross validation tells us that the best parameters are 5 neighbours with a weighted distance.\n",
    "\n",
    "With this, we get an accuracy of 94.14% +/- 0.83% which is a lower accuracy and higher standard deviation as compared to decision tree classifier.\n",
    "\n",
    "The f1-score varies for various classes and the micro avg f1 score is 96% which is lower than Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM\n",
    "\n",
    "For support vector classification, we want to check if linear or rbf kernal provides us a better perfromance so I am running a nested cross validation with grid search.\n",
    "\n",
    "### Tuning parameters - Nested CV & Grid search\n",
    "\n",
    "To search for the best parameters, we run a nested cross validation with grid search with the following hyper parameters\n",
    "\n",
    "1. C: This heps us introduce soft margin to the SVM classification, allowing for outliers. Larger the C, smaller number of errors are allowed. It is a way of controlling overfitting - by making C smaller, we allow some outliers\n",
    "\n",
    "2. gamma: For the gaussian rbf kernel trick, we cna specify the number of neighbours that the svm classifier will look at for deciding the boundaires. A large gamma value indicates looking at few neighbours and cna lead to overfitting the model. A small gamma value leads to looking at a large number of neighbours.\n",
    "\n",
    "3. kernel trick: for data points that are linearly seperable, we can use the linear kernel and rbf kernel is for guassian distribution. We add these to the gridsearch to see which is better for our classification problem.\n",
    "\n",
    "4. decision_function_shape: Since this is a multiclass classification, we need to specify to SVC whether we want the classes compared one over the rest or one over one so we use this also in our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\harsh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\harsh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, object were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# standardization for svc\n",
    "sc = StandardScaler()\n",
    "x_train_t = sc.fit_transform(x_train_t)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "kernels = ['linear','rbf']\n",
    "dfs = ['ovr','ovo']\n",
    "param_grid = {'kernel':kernels,'C': Cs, 'gamma' : gammas, 'decision_function_shape' :dfs}\n",
    "\n",
    "model = SVC(class_weight=\"balanced\",probability=True,random_state = 45)\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_svm = GridSearchCV(model, param_grid, cv = inner_cv, scoring='f1_micro', n_jobs = -1)\n",
    "grid_svm.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_svm, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 97.97% +/- 0.61%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'C': 100, 'decision_function_shape': 'ovr', 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "The best SVM classifier is :-\n",
      " SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=45, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Confusion Matrix: - \n",
      " [[238   4   0   0]\n",
      " [  1  76   0   0]\n",
      " [  0   1  13   0]\n",
      " [  0   0   0  13]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       242\n",
      "           1       0.94      0.99      0.96        77\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      1.00      1.00        13\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       346\n",
      "   macro avg       0.98      0.97      0.98       346\n",
      "weighted avg       0.98      0.98      0.98       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_svm.best_params_)\n",
    "print()\n",
    "print (\"The best SVM classifier is :-\\n\", grid_svm.best_estimator_)\n",
    "y_pred = grid_svm.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "SVM with nested cross validation tells us that the best parameters are 'C': 100, 'decision_function_shape': 'ovr', 'gamma': 0.1, 'kernel': 'rbf'\n",
    "\n",
    "With this, we get an accuracy of 97.97% +/- 0.61% which is the best so far.\n",
    "\n",
    "The f1-score varies for various classes but highest so far. It is pretty great for classes 0,3 but only good for 1,2. However, the micro avg f1 score is 98% which is the best so far.\n",
    "\n",
    "### Prediction between classes\n",
    "\n",
    "1. Class 0 has a high precision and recall of 100% and 98% respectively. There are only 4 unacceptable cars missclassified  which is okay considering 238 are identified correctly.\n",
    "2. Most cars in  Class 1 were identified correctly, with recall = 99% but there are 5 cars identified as class 1 where they are not.\n",
    "3. Class 2 has only 1 missclassified car .\n",
    "4. This Model has predicted well for class 3, with all 13 identified correctly, and none identified inaccurately as class 3.\n",
    "\n",
    "Therefore, SVM predicts really well for all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7756874095513748\n",
      "Testing Accuracy:  0.6994219653179191\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[242   0   0   0]\n",
      " [ 77   0   0   0]\n",
      " [ 14   0   0   0]\n",
      " [ 13   0   0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "# feeding the training data into the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predicting the values for x-test\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# finding the training and testing accuracy\n",
    "print(\"Training Accuracy: \",model.score(x_train, y_train))\n",
    "print(\"Testing Accuracy: \", model.score(x_test, y_test))\n",
    "print()\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a very low accuracy compared to all the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SInce the Decision Tree Classifier and SVM have provided us the best results so far, with SVM having a higher accuracy and f1-score but decision tree having a more smooth standard deviation of accuracy in nested cross validation, we do one hot encoding on the dataset and run it on both these classifiers again using nested cross validation GridSearch.\n",
    "\n",
    "When we do this transformation, we get an increased number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('car.data', header = None)\n",
    "data.columns = ['Price', 'Maintenance Cost', 'Number of Doors', 'Capacity', 'Size of Luggage Boot', 'safety', 'Decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(categories='auto')\n",
    "enc.fit(data.iloc[:,:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['high', 'low', 'med', 'vhigh'], dtype=object),\n",
       " array(['high', 'low', 'med', 'vhigh'], dtype=object),\n",
       " array(['2', '3', '4', '5more'], dtype=object),\n",
       " array(['2', '4', 'more'], dtype=object),\n",
       " array(['big', 'med', 'small'], dtype=object),\n",
       " array(['high', 'low', 'med'], dtype=object)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c = pd.DataFrame(enc.transform(data.iloc[:,:6]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   11   12   13   14  \\\n",
       "0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "\n",
       "    15   16   17   18   19   20  \n",
       "0  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  1.0  0.0  0.0  1.0  \n",
       "2  0.0  0.0  1.0  1.0  0.0  0.0  \n",
       "3  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "4  0.0  1.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Decision'].replace(('unacc', 'acc', 'good', 'vgood'), (0, 1, 2, 3), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c = data['Decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting 20% aside as testing data for final evaluation of our model\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_c, y_c, test_size = 0.20, random_state = 45, stratify = y_c)\n",
    "\n",
    "#setting 20% aside as validation data for cross validation\n",
    "x_train_t, x_train_v, y_train_t, y_train_v = train_test_split(x_train, y_train, test_size = 0.20, random_state = 45, stratify = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM (On Categorically encoded data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SVM Classification on the one hot encoded data using grid search cv and nested cv and the same grid search hyper parameters as we did for the numeric encoded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "kernels = ['linear','rbf']\n",
    "dfs = ['ovr','ovo']\n",
    "param_grid = {'kernel':kernels,'C': Cs, 'gamma' : gammas, 'decision_function_shape' :dfs}\n",
    "\n",
    "model = SVC(class_weight=\"balanced\",probability=True,random_state = 45)\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_svm = GridSearchCV(model, param_grid, cv = inner_cv, scoring='f1_micro', n_jobs = -1)\n",
    "grid_svm.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_svm, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 99.71% +/- 0.29%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'C': 100, 'decision_function_shape': 'ovr', 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "The best SVM classifier is :-\n",
      " SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=45, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[240   1   2   0]\n",
      " [  0  74   0   0]\n",
      " [  0   0  15   0]\n",
      " [  0   0   0  14]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       243\n",
      "           1       0.99      1.00      0.99        74\n",
      "           2       0.88      1.00      0.94        15\n",
      "           3       1.00      1.00      1.00        14\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       346\n",
      "   macro avg       0.97      1.00      0.98       346\n",
      "weighted avg       0.99      0.99      0.99       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_svm.best_params_)\n",
    "print()\n",
    "print (\"The best SVM classifier is :-\\n\", grid_svm.best_estimator_)\n",
    "y_pred = grid_svm.best_estimator_.predict(x_test)\n",
    "print()\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "SVM on one hot encoded data with nested cross validation tells us that the best parameters are 'C': 100, 'decision_function_shape': 'ovr', 'gamma': 0.1, 'kernel': 'rbf'\n",
    "\n",
    "With this, we get an accuracy of 99.71% +/- 0.29% which is the best so far, similar to the SVM classifier which used the ordinal data as numeric data. We have also reduced the standard deviation to 0.29% which is great.\n",
    "\n",
    "The f1-score is equally good for all classes, but relatively lower for class 2 at 94%.\n",
    "\n",
    "### Prediction between classes\n",
    "\n",
    "1. Classes 0 (unacceptable), 1 (acceptable) and 3 (very good) have a very high precision and recall so this model works really well for all these classes.\n",
    "2. Class 2 (good) has a 100% recall, which means that all good cars were identified correctly but it has a lower precision than other classes (88%) which means that some cars were misclassified as class 2. From the confusion matrix, we can see that 2 cars that are actually class 0 (unacceptable) are classified as good (class 2) which is not good for the business.\n",
    "\n",
    "SVM with One hot encoding is generally good, with some misclassification. But this is the best model we have gotten so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decision Tree (On Categorically encoded data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Decision Tree CLassifier on the one hot encoded data using grid search cv and nested cv and the same grid search hyper parameters as we did for the numeric encoded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(criterion = [\"gini\", \"entropy\"], \n",
    "                  max_depth = range(2,10),\n",
    "                  min_samples_leaf = range(2,8),\n",
    "                  min_impurity_decrease = [0,1e-8,1e-7,1e-6,1e-5,1e-4]\n",
    "                 )\n",
    "\n",
    "grid_tree_clf = tree.DecisionTreeClassifier(class_weight=\"balanced\",random_state=45)\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_tree = GridSearchCV(grid_tree_clf, param_grid, cv = inner_cv, scoring='f1_micro', n_jobs = -1)\n",
    "grid_tree.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_tree, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 94.57% +/- 2.02%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'criterion': 'entropy', 'max_depth': 9, 'min_impurity_decrease': 0, 'min_samples_leaf': 2}\n",
      "\n",
      "The best decision tree classifier is :-\n",
      " DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "            max_depth=9, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=45,\n",
      "            splitter='best')\n",
      "Confusion Matrix: - \n",
      " [[234   7   2   0]\n",
      " [  0  71   1   2]\n",
      " [  0   0  14   1]\n",
      " [  0   0   0  14]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       243\n",
      "           1       0.91      0.96      0.93        74\n",
      "           2       0.82      0.93      0.87        15\n",
      "           3       0.82      1.00      0.90        14\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       346\n",
      "   macro avg       0.89      0.96      0.92       346\n",
      "weighted avg       0.97      0.96      0.96       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_tree.best_params_)\n",
    "print()\n",
    "print (\"The best decision tree classifier is :-\\n\", grid_tree.best_estimator_)\n",
    "y_pred = grid_tree.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "For decision trees on one hot encoded data, the best parameters are 'criterion': 'entropy', 'max_depth': 9, 'min_impurity_decrease': 0, 'min_samples_leaf': 2. But the accuracy and f1-scores are lesser compared to the decision tree classifier with numerical data and lessser than the SVM classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. KNN (Categorical encoding)\n",
    "\n",
    "Running K-Nearest Neighbours classifer of one-hot encoded data\n",
    "\n",
    "### Modeling and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1,30))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "param_grid = dict(n_neighbors = k_range, weights = weight_options)\n",
    "\n",
    "grid_knn_clf = neighbors.KNeighborsClassifier()\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_knn = GridSearchCV(grid_knn_clf, param_grid, cv = inner_cv, scoring='f1_micro', n_jobs = -1)\n",
    "grid_knn.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_knn, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 89.36% +/- 0.64%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'n_neighbors': 8, 'weights': 'distance'}\n",
      "\n",
      "The best KNN classifier is :-\n",
      " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=8, p=2,\n",
      "           weights='distance')\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[238   3   2   0]\n",
      " [ 14  60   0   0]\n",
      " [  0   6   9   0]\n",
      " [  0   5   0   9]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       243\n",
      "           1       0.81      0.81      0.81        74\n",
      "           2       0.82      0.60      0.69        15\n",
      "           3       1.00      0.64      0.78        14\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       346\n",
      "   macro avg       0.89      0.76      0.81       346\n",
      "weighted avg       0.91      0.91      0.91       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_knn.best_params_)\n",
    "print()\n",
    "print (\"The best KNN classifier is :-\\n\", grid_knn.best_estimator_)\n",
    "y_pred = grid_knn.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print()\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "For knn on one hot encoded data, the best parameters are 'n_neighbors': 8, 'weights': 'distance'. But the accuracy and f1-scores are lesser compared to the knn with numerical data and lessser than the SVM classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Logitic (Categorical Encoding)\n",
    "\n",
    "Running Logistic regression classifier on one hot encoded data\n",
    "\n",
    "### Modeling and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \\\n",
    "              'C':[1e-4,0.001,.009,0.01,.09,1,5,10,25,100,1000,1e4]}\n",
    "\n",
    "clf = linear_model.LogisticRegression(class_weight=\"balanced\", multi_class='auto', penalty = 'l2', random_state=45)\n",
    "\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=45)\n",
    "\n",
    "grid_logit = GridSearchCV(clf, grid_values, cv = inner_cv, scoring='f1_micro',n_jobs=-1)\n",
    "grid_logit.fit(x_train_t,y_train_t)\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(grid_logit, x_train, y_train, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Nested CV with grid search, we acheive accuracy: 93.71% +/- 1.11%\n",
      "\n",
      "The best hyper-parameters to get these accuracy are :-\n",
      " {'C': 100, 'solver': 'lbfgs'}\n",
      "\n",
      "The best logistic regression classifier is :-\n",
      " LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='auto', n_jobs=None, penalty='l2', random_state=45,\n",
      "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)\n",
      "Confusion Matrix: - \n",
      " [[224  17   2   0]\n",
      " [  0  70   2   2]\n",
      " [  0   0  15   0]\n",
      " [  0   0   0  14]]\n",
      "\n",
      "Classification Report: - \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       243\n",
      "           1       0.80      0.95      0.87        74\n",
      "           2       0.79      1.00      0.88        15\n",
      "           3       0.88      1.00      0.93        14\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       346\n",
      "   macro avg       0.87      0.97      0.91       346\n",
      "weighted avg       0.94      0.93      0.94       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Nested CV with grid search, we acheive accuracy: {0:.2%} +/- {1:.2%}\".format(nested_score.mean(), nested_score.std()))\n",
    "print()\n",
    "print (\"The best hyper-parameters to get these accuracy are :-\\n\", grid_logit.best_params_)\n",
    "print()\n",
    "print (\"The best logistic regression classifier is :-\\n\", grid_logit.best_estimator_)\n",
    "y_pred = grid_logit.best_estimator_.predict(x_test)\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=bc.target_names,title='Confusion matrix')\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(\"Classification Report: - \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit\n",
    "\n",
    "For logistic regression on one hot encoded data, the best parameters are 'C': 100, 'solver': 'newton-cg'. With this we get an accuracy of 93.71% +/- 1.11% and f1 score of 93% which is better than logistic regression with numerically encoded data, but the accuracy and f1-scores are lesser compared to the SVM and Decision Tree classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Naive Bayes (Categorical Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8031837916063675\n",
      "Testing Accuracy:  0.8005780346820809\n",
      "\n",
      "Confusion Matrix: - \n",
      " [[184  54   5   0]\n",
      " [  0  65   5   4]\n",
      " [  0   0  14   1]\n",
      " [  0   0   0  14]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "# feeding the training data into the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predicting the values for x-test\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# finding the training and testing accuracy\n",
    "print(\"Training Accuracy: \",model.score(x_train, y_train))\n",
    "print(\"Testing Accuracy: \", model.score(x_test, y_test))\n",
    "print()\n",
    "print(\"Confusion Matrix: - \\n\",confusion_matrix(y_test, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 80.05% testing accuracy with Naive bayes with one hot encoded data which is higher than what we got with numerically encoded data but much lower than other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Categorical and Numberical encoding for Ordinal Data\n",
    "   \n",
    "Treating Ordinal data as Numerical and Categorical have their pros and cons.\n",
    "   \n",
    "| Comparison                                | Numerical Encoding                                           | Categorical (One Hot Encoding)                               |\n",
    "| :---------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n",
    "| Pros                                      | 1. The order of the classes is preserved based on business knowledge.<br />2. There are lesser number of features that the model has to work.<br />3. Due to lesser number of features, the model can be easily explained. | 1. Treating ordinal data as categorical results in treating them as separate categories and hence the differences between two classes does not have a meaning which is useful in some business cases.<br />2.We can be sure we are not overstating the effect of ordering. This is a more conservative approach |\n",
    "| Cons                                      | We can not be sure that the difference between the numbers assigned to the classes are meaningful. <br />For example, if 1 is bad, 2 is good & 3 is vgood, the difference between good and bad is not the same and difference between vgood and good but numerical data would say that the difference is the same. | 1. The number of features increases to a large extent resulting in a complex model.<br />2. We are unable to answer the order of the results. |\n",
    "| Better for models (Based on this dataset) | 1. Decision Tree<br />2. K-Nearest Neighbors                 | 1. SVM<br />2. Logistic Regression<br />3. Nave Bayes       |\n",
    "\n",
    "For some models, we find that one-hot encoding is better and for other numerical encoding is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing all the models that have been analyzed so far:-**\n",
    "\n",
    "| Ordinal Data Treatment             |        Model        |     Best Parameters <br />(Using NestedCV with GridSearchCV) |                        Accuracy | Micro-avg F1 Score | Class-wise Performance                                       |\n",
    "| :--------------------------------- | :-----------------: | -----------------------------------------------------------: | ------------------------------: | -----------------: | ------------------------------------------------------------ |\n",
    "| Numerical Encoding             |  Decision Tree  | criterion: gini<br />max_depth: 9<br />min_impurity_decrease: 0<br /> min_samples_leaf: 2 |            96.89% +/- 0.74% |                97% | **Great for :** Unacceptable<br />**Good for :** acceptable, good, vgood |\n",
    "| Numerical Encoding                 | K Nearest Neighbors |                       n_neighbors: 5<br /> weights: distance |                94.14% +/- 0.83% |                96% | **Great for :** Unacceptable<br />**Good for :** acceptable<br />**Bad for :** good, vgood |\n",
    "| Numerical Encoding                 | Logistic Regression |                C: 25<br />penalty: l2<br />solver: newton-cg |                83.00% +/- 1.02% |                85% | **Good for :** Unacceptable<br />**Bad for :** acceptable, good, vgood |\n",
    "| Numerical Encoding             |         SVM         | kernel: rbf <br />C: 100<br /> gamma: 0.1<br />decision_function_shape: ovr |                97.97% +/- 0.61% |            98% | **Great for :** All Classes                                  |\n",
    "| Numerical Encoding                 |     Nave Bayes     |                                                         *NA* | <br />Testing Accuracy = 69.94% |               *NA* | Not good for any class                                       |\n",
    "| **Categorical (One hot encoding)** |       **SVM**       | kernel: rbf <br />C: 100<br /> gamma: 0.1<br />decision_function_shape: ovr |            **99.71% +/- 0.29%** |            **99%** | **Great for :** All classes                                  |\n",
    "| Categorical (One hot encoding)     |    Decision Tree    | criterion: gini<br />max_depth: 9<br />min_impurity_decrease: 0<br /> min_samples_leaf: 3 |                94.57% +/- 2.02% |                96% | **Great for :** Unacceptable<br />**Good for :** acceptable, vgood<br />**Bad for :** good |\n",
    "| Categorical (One hot encoding)     | K Nearest Neighbors |                       n_neighbors: 8<br /> weights: distance |                89.36% +/- 0.64% |                91% | **Good for :** Unacceptable<br />**Bad for :** acceptable, good, vgood |\n",
    "| Categorical (One hot encoding)     | Logistic Regression |                              C: 100<br />solver: 'newton-cg' |                93.71% +/- 1.11% |                93% | **Great for :** Unacceptable, very good<br />**Okay for :** acceptable, vgood<br /> |\n",
    "| Categorical (One hot encoding) | Nave Bayes | *NA* | Testing Accuracy:  80.05% | *NA* | Not good for any class |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above process, we get the **best model with One Hot Encoding in SVM Classifier**. Even though SVM classifer with Numerical encoding gives a higher average accuracy, the standard deviation in accuracy is lower with one hot encoding and the micro f1 score is also higher at 99%. Overall SVM with One hot encoding gives us better results so we chose that as the best model, but for any reason we need to save the order of the features, we can use SVM classifier with Numerical encoding.\n",
    "\n",
    "**The best model selected is the SVM Classifier** as it has a great performance for all the classes with an accuracy of **99.71% +/- 0.29%** and **micro avg f1 score of 99%** when the Ordinal data is treated as categorical data with one hot encoding. Micro averaged f1 score tunes grid search to predict well for all classes as it calculates metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "This classifier gives us a **precision and recall of 99%+** for all the classes except class 2. Classes 0 (unacceptable), 1 (acceptable) and 3 (very good) have a very high precision and recall so this model works really well for all these classes.\n",
    "Class 2 (good) has a 100% recall, which means that all good cars were identified correctly but it has a lower precision than other classes (88%) which means that some cars were misclassified as class 2. From the confusion matrix, we can see that 2 cars that are actually class 0 (unacceptable) are classified as good (class 2) which is not good for the business.\n",
    "\n",
    "**Based on the above, we select SVM classifier as the best model.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
